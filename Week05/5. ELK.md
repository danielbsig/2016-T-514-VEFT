# The ELK stack #

# What is ELK?
The ELK stack consists of ElasticSearch, Logstash and Kibana. This are all separate open source tools that is common to use for parsing, storing and displaying logs.

Today, these three things have been connected more closely together to one thing called the *Elastic Stack* (https://www.elastic.co/v5). But as for now it is only available as a release candidate and should thus not be used in production yet.

It is possible to use ELK with Docker or just install it locally on your machine, instructions for Ubuntu 16.04 can be found here: https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elk-stack-on-ubuntu-16-04.

# Elastic Search
Full-text search engine based on Lucene, distributed and RESTful service. Very popular due to how fast and scalable it is. Works basically that it performs an action called "indexing", i.e. it takes raw data and transforms it to inner documents and stores in format similar to JSON. It is easy to add documents and search in them, simply performed with a HTTP GET/POST command with JSON body.

## Usage
ElasticSearch uses port 9200 by default. Possible to run it through commandline with curl, example: *"curl localhost:9200/_cat/indices"* shows all indices in ElasticSearch. The command *"curl localhost:9200/_search?q=hello&size=5"* searches for 'hello' in all fields in all documents and returns at most 5 results.  See more at: https://www.elastic.co/guide/en/elasticsearch/reference/current/_introducing_the_query_language.html.

## Download
https://www.elastic.co/downloads/elasticsearch  
Official docker-hub image to use with docker: https://hub.docker.com/_/elasticsearch/

# Logstash
Used for gathering, parsing and forwarding logs. Powerful tool to sorting logs, adding metadata on top of certain categories, together with more options that can be useful when working with logs.

## Config file
Logstash has a special config file that specifies where the logs come from, how they should be parsed and where to send them after parsing. The file is divided into three parts:

- input {}  
Where do the logs come from (From a text file, stdin, a database etc). Usually a plugin is used according to where the logs come from e.g. a file plugin if read from a file. A list of input plugins can be found here: https://www.elastic.co/guide/en/logstash/current/input-plugins.html. Also it is possible to create an input plugin for using.

- filter {}  
How to filter/parse the logs. Commonly *grok* is used. Grok is build from regex, it has ower 100 pre-defined patterns that can be found here: https://github.com/elastic/logstash/blob/v1.4.2/patterns/grok-patterns. Also possible to define new patterns by using regex.  
With grok it is possible to add new tags, fields and more stuff to certain kinds of logs before sending them to Elastic Search. It thus offers an opportunity to add data to logs that will be saved with them and can be queried for through Elastic Search. For example logs can be sorted after severity, category and more. It is also possible to use *mutate* to convert the value of a field to another type, e.g. from a string to a number and also to exchange a letter for another one, either in a certain field or the whole log.  
Two tools that come in very handy when writing a grok filter are the grok debugger: https://grokdebug.herokuapp.com/ and the grok constructor: http://grokconstructor.appspot.com/.  
There are many filter-plugins available (https://www.elastic.co/guide/en/logstash/current/filter-plugins.html) but also possible to write new plugins.
See more at https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html.   

- output {}  
Where to send the logs after filtering them - usually Elastic Search. But as before there are many plugins available (https://www.elastic.co/guide/en/logstash/current/output-plugins.html) and possible to write new plugins.  

An example of logstash configuration files can be found here: https://www.elastic.co/guide/en/logstash/current/config-examples.html.

## Usage
Flags:
* -f Get logstash config file from certain path

*logstash -f /etc/logstash/conf.d/logstash.conf* - runs logstash with config file from path defined.

## Download
https://www.elastic.co/downloads/logstash  
Official docker-hub image to use with docker: https://hub.docker.com/_/logstash/

# Kibana
Tool to visualize search results from Elastic Search. Easy to use but really powerful. Possible to make all kinds of charts based on search results under the tab *Visualize*.
Possible to make your own dashboard with various information on, that can then be set to update/refresh regularly (you choose a time interval).  
All management is interactive and visual, i.e. when searching in Elastic Search the query is written in a search field on the page (nothing done on command line).

## Usage
Kibana uses default port 5601. It can be reached by going to *localhost:5601* in your browser. If Elastic Search is running behind Kibana then an interface appears that asks you to save indices if they are present and after that it is possible to work with the data.
If Elastic Search is running but there are no indices (for example if logstash is not working the way it should) then a display appears that says that unfortunately no indices where found.  
There are a few plugins available for Kibana (https://github.com/elastic/kibana/wiki/Known-Plugins) and it is also possible to write new plugins but that process is not particulary well supported in the current versions.

## Download
https://www.elastic.co/downloads/kibana  
Official docker-hub image to use with docker: https://hub.docker.com/_/kibana/

## Continue
[Week 6](../Week06)
